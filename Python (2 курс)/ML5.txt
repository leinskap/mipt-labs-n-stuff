# ANN (Approximate nearest neighbor)
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris_dataset = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)


1. Построить K-d tree
1.1 Выбрать первый столбец у X_train
1.2 Найти медиану
1.3 Разбить X_train на две части, где больше медианы и меньше или равно медиане
1.4 С каждой половиной проделать 1.1 до 1.3 взяв второй столбец. И так до четвёртого столбца

1.5 Используйте класс
class Node():
  def __init__(self):
    self.num_column = None
    self.median = None
    self.right = None
    self.left = None
    self.arr = None

def f(node, arr, num_column=0):
  node.num_column = num_column
  if num_column == 4:
    node.arr = arr
    return 0
  # code here!

root = Node()
f(root, X_train)

root.right.right.right.right.arr
array([[6.7, 3.3, 5.7, 2.5],
       [7.2, 3.6, 6.1, 2.5],
       [6.3, 3.4, 5.6, 2.4],
       [6.7, 3.1, 5.6, 2.4]])

root.right.right.right.left.arr
array([[7.2, 3.2, 6. , 1.8],
       [6.9, 3.2, 5.7, 2.3],
       [6.7, 3.3, 5.7, 2.1],
       [6.8, 3.2, 5.9, 2.3],
       [7.9, 3.8, 6.4, 2. ],
       [7.7, 3.8, 6.7, 2.2]])

root.right.right.right.median
#2.3


2. Построить Ball tree
2.1 Найти центр масс через np.mean
2.2 Найти самую дальнюю точку от центра
2.3 Найти самую дальнюю точку от точки 2.2
2.4 Разбить X_train на две часть, исходя из близости к точкам 2.2 и 2.3
2.5 Найти центры масс в каждой из них


3. Построить HNSW с 3 слоями
3.1 У каждой точки найти 5 ближайших соседей (создать граф)
3.2 Взять каждую 4 точку в X_train и повторить 3.1
3.3 Взять каждую 4 точку в X_train из 3.2 и повторить 3.1


4. Написать поиск по HNSW и посчитать accuracy
accuracy_score(y_test, y_pred)


5. Построить график времени инициализации и предсказаний
5.1 Замерить время построения графа
from time import time as t
_ = t()
p.add_items(data)
print(t()-_)
5.2 Замерить время поиска
5.3 Меняйте размер датасета от 5 000 до 100 000

#https://github.com/nmslib/hnswlib
!apt-get install -y python-setuptools python-pip
!git clone https://github.com/nmslib/hnswlib.git
!cd hnswlib && pip install .

import hnswlib
import numpy as np

dim = 16
num_elements = 10000

# Generating sample data
data = np.float32(np.random.random((num_elements, dim)))

# We split the data in two batches:
data1 = data[:num_elements // 2]
data2 = data[num_elements // 2:]

# Declaring index
p = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip

# Initializing index
# max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded
# during insertion of an element.
# The capacity can be increased by saving/loading the index, see below.
#
# ef_construction - controls index search speed/build speed tradeoff
#
# M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)
# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction

p.init_index(max_elements=num_elements//2, ef_construction=100, M=16)

# Controlling the recall by setting ef:
# higher ef leads to better accuracy, but slower search
p.set_ef(10)

# Set number of threads used during batch search/construction
# By default using all available cores
p.set_num_threads(4)

print("Adding first batch of %d elements" % (len(data1)))
p.add_items(data1)

# Query the elements for themselves and measure recall:
labels, distances = p.knn_query(data1, k=1)
print("Recall for the first batch:", np.mean(labels.reshape(-1) == np.arange(len(data1))), "\n")

# Serializing and deleting the index:
index_path='first_half.bin'
print("Saving index to '%s'" % index_path)
p.save_index("first_half.bin")
del p

# Re-initializing, loading the index
p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.

print("\nLoading index from 'first_half.bin'\n")

# Increase the total capacity (max_elements), so that it will handle the new data
p.load_index("first_half.bin", max_elements = num_elements)

print("Adding the second batch of %d elements" % (len(data2)))
p.add_items(data2)

# Query the elements for themselves and measure recall:
labels, distances = p.knn_query(data, k=1)
print("Recall for two batches:", np.mean(labels.reshape(-1) == np.arange(len(data))), "\n")